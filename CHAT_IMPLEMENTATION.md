# Chat Implementation - Lebensessenz Kursbot

## √úbersicht

Das System wurde von einem einfachen Q&A-Bot zu einem vollwertigen Chat mit Kontext erweitert.

### Key Features

- **Rolling Summary**: Automatische Zusammenfassung der Konversation
- **Kontext-Bewusstsein**: Letzte N Nachrichten (default: 8) werden in Anfragen einbezogen
- **Standalone Query Rewriting**: Referenzen wie "das" oder "und noch" werden aufgel√∂st
- **Grounded Responses**: Antworten basieren strikt auf Kurs-Snippets
- **Persistenz**: SQLite-basiert, Conversations bleiben nach Reload erhalten
- **localStorage**: ConversationID wird im Browser gespeichert

## Architektur

### Datenmodell

**Conversations:**
```sql
- id (TEXT, PRIMARY KEY)
- created_at (TEXT)
- updated_at (TEXT)
- summary_text (TEXT, nullable)
- summary_updated_at (TEXT, nullable)
- summary_message_cursor (INTEGER, default 0)
```

**Messages:**
```sql
- id (TEXT, PRIMARY KEY)
- conversation_id (TEXT, FOREIGN KEY)
- role (TEXT: 'user' | 'assistant')
- content (TEXT)
- created_at (TEXT)
```

### API Endpoints

**POST /chat**
```json
Request:
{
  "conversationId": "uuid-optional",
  "message": "user question"
}

Response:
{
  "conversationId": "uuid",
  "answer": "assistant response",
  "sources": [...]
}
```

**GET /conversations/:id/messages**
```json
Response:
{
  "messages": [
    {
      "id": "uuid",
      "conversation_id": "uuid",
      "role": "user|assistant",
      "content": "message text",
      "created_at": "ISO timestamp"
    },
    ...
  ]
}
```

**GET /health**
```json
Response:
{
  "ok": true
}
```

### Chat Flow

1. **Conversation Creation**: Wenn keine `conversationId` vorhanden ‚Üí neue Conversation anlegen
2. **Message Storage**: User-Message speichern
3. **Context Loading**: Summary + letzte N Messages laden
4. **Query Rewriting**: Optional standalone query aus Kontext generieren
5. **Retrieval**: Top-K (default: 10) Kurs-Snippets abrufen
6. **LLM Generation**: Response mit System Instructions, Summary, Last Messages, Snippets
7. **Save Response**: Assistant-Message speichern
8. **Summary Update**: Wenn Threshold erreicht (default: 6 neue Messages) ‚Üí Summary aktualisieren

### Rolling Summary

**Trigger:**
- Beim ersten Mal: Nach mindestens 4 Messages (2 Turns)
- Danach: Alle 6 neuen Messages seit letztem Summary

**Prozess:**
1. Alte Summary + neue Messages seit Cursor laden
2. LLM generiert kompakte neue Summary (3-4 S√§tze)
3. Cursor auf aktuelle Message-Anzahl setzen
4. Summary in DB speichern

**Zweck:**
- Reduziert Token-Usage f√ºr lange Conversations
- Erh√§lt Kontext ohne volle Historie zu senden
- Erm√∂glicht Referenzen √ºber viele Turns hinweg

## Konfiguration

### Environment Variables

```bash
# OpenAI
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBED_MODEL=text-embedding-3-small

# ChromaDB
CHROMA_DIR=storage/chroma
COLLECTION_NAME=kursmaterial_v1

# Chat Parameters
TOP_K=10                    # Course snippets to retrieve
LAST_N=8                    # Last N messages to include
SUMMARY_THRESHOLD=6         # Update summary every N messages
MAX_CONTEXT_CHARS=9000      # Max chars for course snippets

# Database
DB_PATH=storage/chat.db
```

### Anpassungen

**Mehr Kontext:**
```bash
LAST_N=12
TOP_K=15
```

**H√§ufigere Summaries:**
```bash
SUMMARY_THRESHOLD=4
```

**L√§ngere Kurs-Snippets:**
```bash
MAX_CONTEXT_CHARS=12000
```

## Dateien

### Neu
- `app/database.py` - SQLite database layer
- `app/chat_service.py` - Chat logic mit Rolling Summary
- `test_chat.py` - Smoke test script
- `CHAT_IMPLEMENTATION.md` - Diese Dokumentation

### Modifiziert
- `app/main.py` - FastAPI app mit Chat endpoints + UI
- `app/__init__.py` - Package marker

### Backup
- `app/main_old_backup.py` - Original Q&A version

## Start & Test

### Server starten
```bash
cd ~/Documents/lebensessenz-kursbot
source .venv/bin/activate
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Browser √∂ffnen
```
http://localhost:8000
```

### Smoke Test
```bash
# In separatem Terminal
source .venv/bin/activate
python test_chat.py
```

### Manueller Test Flow

1. **Turn 1**: "Nenn mir die Kernpunkte von Seite 4"
   - Erwartung: Bot gibt Kernpunkte aus Seite 4 zur√ºck
   - ConversationID wird in localStorage gespeichert

2. **Turn 2**: "Und wie war das mit Milchprodukten?"
   - Erwartung: Bot versteht "wie war das" aus Kontext
   - Query wird rewritten zu standalone query
   - Antwort bleibt grounded in Kurs-Material

3. **Reload Page**
   - Erwartung: Chat-Historie wird geladen
   - Conversation kann fortgesetzt werden

4. **New Chat Button**
   - Erwartung: Neuer Chat startet
   - Alter Chat bleibt in DB gespeichert

## Sicherheit & Qualit√§t

### System Instructions

Der Bot erh√§lt klare Regeln:
1. **Faktenbasis**: Nur Kurs-Snippets sind Faktenquelle
2. **Kontext f√ºr Referenzen**: Historie nur f√ºr "das", "wie vorhin" etc.
3. **Grenzen kommunizieren**: "nicht im Material" wenn Info fehlt
4. **Keine Spekulation**: Keine erfundenen Fakten
5. **Keine Medizin**: Keine Diagnosen/Behandlungsempfehlungen

### Standalone Query Rewriting

Beispiele:
```
Turn 1: "Was ist die 50/50-Regel?"
‚Üí Query: "Was ist die 50/50-Regel?"

Turn 2: "Und wie war das mit Obst?"
‚Üí Query: "50/50-Regel und Obst in Lebensmittelkombination"

Turn 3: "Gibt es Ausnahmen?"
‚Üí Query: "Gibt es Ausnahmen bei der 50/50-Regel f√ºr Lebensmittelkombinationen?"
```

## Troubleshooting

### "ModuleNotFoundError: No module named 'app'"
```bash
# Stelle sicher, dass du im Projekt-Root bist
cd ~/Documents/lebensessenz-kursbot
# Installiere Package im development mode
pip install -e .
# Oder starte mit:
python -m uvicorn app.main:app --reload
```

### "Conversation not found"
- ConversationID in localStorage ist ung√ºltig
- L√∂sung: "New Chat" klicken oder localStorage clearen

### "No module named 'requests'" (f√ºr test)
```bash
pip install requests
```

### Summary wird nicht erstellt
- Check: Mindestens 4 Messages (2 Turns) vorhanden?
- Check: SUMMARY_THRESHOLD erreicht?
- Logs pr√ºfen f√ºr Fehler

## Performance

### Token Usage

**Ohne Summary** (nach 20 Turns):
- ~8 Messages √ó ~200 tokens = 1600 tokens/request
- + 10 Snippets √ó ~300 tokens = 3000 tokens
- **Total: ~4600 tokens/request**

**Mit Summary** (nach 20 Turns):
- Summary: ~150 tokens
- 8 Last Messages: ~1600 tokens
- 10 Snippets: ~3000 tokens
- **Total: ~4750 tokens/request**

**Ohne jeglichen Kontext** (alte Version):
- 10 Snippets: ~3000 tokens
- **Total: ~3000 tokens/request**

‚Üí Rolling Summary h√§lt Token-Usage konstant, auch bei langen Conversations

### Latenz

- Query Rewriting: +0.5-1s
- Summary Generation (alle 6 Msgs): +1-2s
- Normale Response: ~2-3s
- **Durchschnitt: ~3s pro Turn**

## Erweiterungen

### M√∂gliche Next Steps

1. **Multi-User Support**: User-IDs, Auth
2. **Conversation Management**: Liste aller Conversations, L√∂schen, Umbenennen
3. **Advanced Retrieval**: Hybrid Search (BM25 + Vector), Reranking
4. **Streaming**: Server-Sent Events f√ºr progressive responses
5. **Citations**: Inline-Quellen im Text
6. **Feedback**: üëç/üëé f√ºr Messages
7. **Export**: Conversation als PDF/Markdown

### Integration in Website

Das Frontend ist bereits im Website-Stil gestaltet. F√ºr volle Integration:

1. **iFrame**: Einfachste Option
```html
<iframe src="http://bot.domain.com" width="100%" height="600px"></iframe>
```

2. **Widget**: Chat als Overlay
```javascript
// Lade chat.js
// Initialisiere mit: new KursbotWidget({ apiUrl: '...' })
```

3. **Native Integration**: Frontend-Code extrahieren und in Website einbauen

## Lizenz & Credits

Implementiert f√ºr Lebensessenz nach Senior Full-Stack + RAG-Engineer Spezifikation.
Rolling Summary Architektur inspiriert von best practices aus Production RAG Systems.
